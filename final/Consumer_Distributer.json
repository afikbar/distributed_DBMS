{"paragraphs":[{"text":"%sh\nsudo service elasticsearch start","user":"anonymous","dateUpdated":"2018-06-27T16:49:01+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Starting elasticsearch: [  OK  ]\r\n"}]},"apps":[],"jobName":"paragraph_1530090217477_-1780724241","id":"20180627-090337_1578451359","dateCreated":"2018-06-27T09:03:37+0000","dateStarted":"2018-06-27T09:17:28+0000","dateFinished":"2018-06-27T09:17:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2481","title":"Start elasticsearch service"},{"title":"My little helpers","text":"%pyspark\n\nfrom kafka import KafkaConsumer\nimport re\nimport json\nimport argparse, elasticsearch, json\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch.helpers import bulk\nfrom datetime import datetime\n\n    \nSCHEMA_DEF = ['id', 'stop_date', 'location_raw', 'driver_gender', \n                'driver_race', 'violation', 'search_conducted', 'is_arrested']\nLEFT_SCHEMA = [fld for fld in SCHEMA_DEF if fld not in ['is_arrested', 'violation','location_raw', 'driver_gender']]\n\ndef msg_cleaner(msg):\n    result = re.sub(\"[()']\",'',msg.value.decode('utf8')).split(\",\")\n    return [word.strip() for word in result]\n\ndef str_to_bool(str):\n    return str.lower() == \"true\"\n\ndef data_filter(msg_dict):#returns true if data is ok\n    return msg_dict['location_raw'].title() in ['San Diego', 'Redding', 'Buttonwillow', 'Modesto'] and\\\n            msg_dict['search_conducted'].lower() in ['true','false'] and\\\n            msg_dict['is_arrested'].lower() in ['true','false'] and\\\n            msg_dict['driver_gender'].lower() in ['m','f'] and\\\n            re.match('(\\d{4})[/.-](\\d{2})[/.-](\\d{2})$', msg_dict['stop_date'])\n                        \ndef data_convert(msg_dict):\n    msg_dict['search_conducted'] = str_to_bool(msg_dict['search_conducted'])\n    msg_dict['is_arrested'] = str_to_bool(msg_dict['is_arrested'])\n    msg_dict['stop_date'] = datetime.strptime(msg_dict['stop_date'], \"%Y-%m-%d\")\n    msg_dict['driver_gender'] = msg_dict['driver_gender'].upper()\n    msg_dict['location_raw'] = msg_dict['location_raw'].title()\n","user":"anonymous","dateUpdated":"2018-06-27T16:48:36+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1530100433828_1334109485","id":"20180627-115353_989705873","dateCreated":"2018-06-27T11:53:53+0000","dateStarted":"2018-06-27T16:36:11+0000","dateFinished":"2018-06-27T16:36:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2482"},{"title":"Def: Consumer & Distributer","text":"%pyspark\n\n# distribute data data into indices in elasticsearch\n# using our distribution method from H.W.1\ndef tal_distributer(es, i, msg_dict):\n    # black panther - redding:\n    if msg_dict['search_conducted'] and \\\n            msg_dict['stop_date'] >= datetime(2016, 1, 30):\n        bp_json = json.dumps(msg_dict, default=str)\n        es.index(index='reddings', doc_type='json', id=i, body=bp_json)\n\n    else:\n        # spiderpig - buttonwillow\n        loc_json = json.dumps({\"location_raw\": msg_dict[\"location_raw\"]}, default=str)\n        es.index(index='buttonwillow_loc', doc_type='json', id=i, body=loc_json)\n\n        # spiderman all genders\n        gender_json = json.dumps({\"driver_gender\": msg_dict[\"driver_gender\"]}, default=str)\n        es.index(index='modesto_gender', doc_type='json', id=i, body=gender_json)\n\n        # superman - San Diego\n        viol_json = json.dumps({\"violation\": msg_dict[\"violation\"]}, default=str)\n\n        if msg_dict['driver_gender'] == 'F':  # all females from bp_left\n            es.index(index='sandiego_viol', doc_type='json', id=i, body=viol_json)\n\n        else:\n            if msg_dict['location_raw'] == 'Modesto':\n                es.index(index='modesto_male_viol', doc_type='json', id=i, body=viol_json)\n            elif msg_dict['location_raw'] == 'San Diego':\n                es.index(index='sandiego_male_viol', doc_type='json', id=i, body=viol_json)\n            elif msg_dict['location_raw'] == 'Redding':\n                es.index(index='redding_male_viol', doc_type='json', id=i, body=viol_json)\n            elif msg_dict['location_raw'] == 'Buttonwillow':\n                es.index(index='buttonwillow_male_viol', doc_type='json', id=i, body=viol_json)\n\n        arstd_json = json.dumps({\"is_arrested\": msg_dict[\"is_arrested\"]}, default=str)\n        # NevadaArrested\n        if msg_dict['location_raw'] != 'Modesto' and msg_dict['driver_gender'] == 'F':\n            es.index(index='sandiego_arrested', doc_type='json', id=i, body=arstd_json)\n\n        # spiderman - Modesto (CaliforniaArrested)\n        else:\n            es.index(index='modesto_arrested', doc_type='json', id=i, body=arstd_json)\n\n        left_json = json.dumps({k: msg_dict[k] for k in LEFT_SCHEMA}, default=str)\n        es.index(index='leftovers', doc_type='json', id=i, body=left_json)\n\n\n# set connection to kafka (by given ip, port & topic)\n# returns a generator of messages and schema (assuming 1st line is schema)\ndef tal_consumer(ip, port, topic):\n    consumer = KafkaConsumer(topic,\n                             bootstrap_servers='{}:{}'.format(ip, port),\n                             auto_offset_reset='earliest')\n\n    first = next(consumer)\n    schema = msg_cleaner(first)\n\n    return consumer, schema\n","user":"anonymous","dateUpdated":"2018-06-27T16:36:14+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530101421343_-195101592","id":"20180627-121021_1228874186","dateCreated":"2018-06-27T12:10:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2483","dateFinished":"2018-06-27T16:36:14+0000","dateStarted":"2018-06-27T16:36:14+0000","results":{"code":"SUCCESS","msg":[]}},{"title":"Get message->Clean->Post ","text":"%pyspark\r\n\r\n\r\ndef main():\r\n    consumer, schema = tal_consumer(\"104.209.178.73\", \"5601\", \"FPA\")\r\n\r\n    es = Elasticsearch()\r\n\r\n    passed_filter = 1\r\n    for i, msg in enumerate(consumer, 1):\r\n\r\n        print \"message number: {}\".format(i)\r\n\r\n        val_ls = msg_cleaner(msg)\r\n\r\n        # keeps only wanted data\r\n        msg_dict = dict([(k, v) for k, v in zip(schema, val_ls) if k in SCHEMA_DEF])\r\n\r\n        if data_filter(msg_dict) == False:\r\n            continue\r\n\r\n        data_convert(msg_dict)\r\n        print \"Passed filters {}\".format(passed_filter)\r\n        passed_filter += 1\r\n\r\n        # go to elastic.......\r\n        tal_distributer(es, i, msg_dict)\r\n\r\n    return\r\n\r\n\r\n#run main\r\nmain()","user":"anonymous","dateUpdated":"2018-06-27T16:47:25+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8758002193080670505.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8758002193080670505.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 14, in <module>\n  File \"<stdin>\", line 2, in main\n  File \"<stdin>\", line 33, in tal_consumer\n  File \"/usr/local/lib/python2.7/site-packages/kafka/consumer/group.py\", line 324, in __init__\n    self._client = KafkaClient(metrics=self._metrics, **self.config)\n  File \"/usr/local/lib/python2.7/site-packages/kafka/client_async.py\", line 221, in __init__\n    self.config['api_version'] = self.check_version(timeout=check_timeout)\n  File \"/usr/local/lib/python2.7/site-packages/kafka/client_async.py\", line 826, in check_version\n    raise Errors.NoBrokersAvailable()\nNoBrokersAvailable: NoBrokersAvailable\n\n"}]},"apps":[],"jobName":"paragraph_1530020311707_1457503984","id":"20180626-133831_2102807295","dateCreated":"2018-06-26T13:38:31+0000","dateStarted":"2018-06-27T16:37:46+0000","dateFinished":"2018-06-27T16:38:26+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:2484"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2018-06-27T16:44:02+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530117842121_-1688007153","id":"20180627-164402_1132204366","dateCreated":"2018-06-27T16:44:02+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2893"}],"name":"Consumer_Distributer","id":"2DHW9B3ZB","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}